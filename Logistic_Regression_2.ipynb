{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3b900e-c2c4-45a0-90b0-5e060480f816",
   "metadata": {},
   "source": [
    "# ANSWER 1\n",
    "GridSearchCV is a technique for finding the optimal parameter values from a given set of parameters in a grid. It's essentially a cross-validation technique.\n",
    "\n",
    "## GridSearchCV work\n",
    "we pass predefined values for hyperparameters to the GridSearchCV function. We do this by defining a dictionary in which we mention a particular hyperparameter along with the values it can take. Here is an example of it\n",
    "\n",
    " {'C':[0.1, 1, 10, 100, 1000],'gamma': [1, 0.1, 0.01, 0.001, 0.0001],'kernel': ['rbf',’linear’,'sigmoid]  }\n",
    " \n",
    "GridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination using the Cross-Validation method. Hence after using this function we get accuracy/loss for every combination of hyperparameters and we can choose the one with the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3e74aa-689a-4e58-a274-c778f22216dd",
   "metadata": {},
   "source": [
    "# ANSWER 2\n",
    "\n",
    "## Grid Search\n",
    "When performing hyperparameter optimization, we first need to define a parameter space or parameter grid, where we include a set of possible hyperparameter values that can be used to build the model.\n",
    "\n",
    "The grid search technique is then used to place these hyperparameters in a matrix-like structure, and the model is trained on every combination of hyperparameter values.\n",
    "\n",
    "The model with the best performance is then selected.\n",
    "\n",
    "GridSearchCV implements the most obvious way of finding an optimal value for anything — it simply tries all the possible values (that you pass) one at a time and returns which one yielded the best model results, based on the scoring that you want, such as accuracy on the test set.\n",
    "## Random Search\n",
    "While grid search looks at every possible combination of hyperparameters to find the best model, random search only selects and tests a random combination of hyperparameters.\n",
    "\n",
    "This technique randomly samples from a grid of hyperparameters instead of conducting an exhaustive search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dda14e-3774-4ab3-8c2e-7bc807f9539c",
   "metadata": {},
   "source": [
    "# ANSWER 3\n",
    "“When data set contains relevant data, but similar data is not obtainable when the models are used for predictions, data leakage (or leaking) occurs. This results in great success on the training dataset (and possibly even the validation accuracy), but lack of performance in production.”\n",
    "\n",
    "When data leakage occurs, it usually leads to overly optimistic outcomes during the model building phase, followed by the unpleasant surprise of poor results after the prediction model is implemented and tested on new data. \n",
    " \n",
    "The most fundamental example of data leakage would be if the true label of a dataset was included as a characteristic in the model. If this object is classified as an apple, the algorithm would learn to predict that it is an apple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aa01d3-da23-4e79-a391-0e6a235b4901",
   "metadata": {},
   "source": [
    "# ANSWER 4\n",
    "To prevent such leakage, the dataset should be separated into training and testing sets before performing any preprocessing steps. The preprocessing steps should only be fit on the training dataset and then applied to both the training and test datasets separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a82631-11df-43ae-a675-d4fac53b75ee",
   "metadata": {},
   "source": [
    "# ANSWER 5\n",
    "A confusion matrix, also known as an error matrix, is a summarized table used to assess the performance of a classification model. The number of correct and incorrect predictions are summarized with count values and broken down by each class.\n",
    "\n",
    "Confusion matrices can help with side-by-side comparisons of different classification methods. You can see not only how accurate one model is over the other, but also see more granularly how a model does in sensitivity or specificity, as those might be more important factors than general accuracy itself.\n",
    "\n",
    "A confusion matrix is a technique for summarizing the performance of a classification algorithm. Classification accuracy alone can be misleading if you have an unequal number of observations in each class or if you have more than two classes in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c612db8-e052-4559-b0b2-2f7314061bf3",
   "metadata": {},
   "source": [
    "# ANSWER 6\n",
    "Precision\n",
    "\n",
    "Out of all the positive predicted, what percentage is truly positive.\n",
    "\n",
    "## precision = TP / (TP+FP)\n",
    "\n",
    "Recall\n",
    "\n",
    "Out of the total positive, what percentage are predicted positive. It is the same as TPR (true positive rate).\n",
    "\n",
    "## recall = TP / (TP+FN)\n",
    "\n",
    "where true positive = TP, true negative = TN, false negative = FN, and false positive FP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082fcec5-18d1-4922-9d0c-69d1170c6033",
   "metadata": {},
   "source": [
    "# ANSWER 7\n",
    "The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.\n",
    "\n",
    "A confusion matrix can be used to evaluate the performance of a machine learning model in a number of ways. One way to use a confusion matrix is to calculate the accuracy of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775deba-fcc8-4a76-8932-86e7fd0d4020",
   "metadata": {},
   "source": [
    "# ANSWER 8\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model by comparing predicted labels to actual labels. From a confusion matrix, several evaluation metrics can be derived:\n",
    "\n",
    "True Positive (TP): The number of instances correctly predicted as the positive class.\n",
    "\n",
    "True Negative (TN): The number of instances correctly predicted as the negative class.\n",
    "\n",
    "False Positive (FP): The number of instances incorrectly predicted as the positive class.\n",
    "\n",
    "False Negative (FN): The number of instances incorrectly predicted as the negative class.\n",
    "\n",
    "Using these basic components, we can calculate various metrics:\n",
    "\n",
    "Accuracy: Accuracy measures the overall correctness of the model's predictions and is calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "\n",
    "Precision: Precision measures the accuracy of the positive predictions and is calculated as TP / (TP + FP). It represents the proportion of positive instances among the predicted positive instances.\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Recall measures the proportion of actual positive instances that were correctly identified by the model. It is calculated as TP / (TP + FN).\n",
    "\n",
    "Specificity (True Negative Rate): Specificity measures the proportion of actual negative instances that were correctly identified by the model. It is calculated as TN / (TN + FP).\n",
    "\n",
    "F1 Score: The F1 score is the harmonic mean of precision and recall and provides a balanced measure of the two metrics. It is calculated as 2 * (Precision * Recall) / (Precision + Recall).\n",
    "\n",
    "False Positive Rate (FPR): FPR measures the proportion of actual negative instances that were incorrectly classified as positive. It is calculated as FP / (FP + TN).\n",
    "\n",
    "These metrics help to assess different aspects of the model's performance and are valuable for understanding its strengths and weaknesses in handling different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84200964-08c5-494d-9266-27485f78c717",
   "metadata": {},
   "source": [
    "# ANSWER 9\n",
    "Accuracy is one of the most common evaluation metrics used to measure the overall performance of a classification model. It represents the proportion of correctly predicted instances (both true positives and true negatives) among all instances. The relationship between accuracy and the values in the confusion matrix can be summarized as follows:\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce039f2c-0ed6-4c83-a42e-224852efda0e",
   "metadata": {},
   "source": [
    "# ANSWER 10\n",
    "A confusion matrix is a valuable tool for identifying potential biases or limitations in a machine learning model, especially in the context of classification tasks. Here are some ways you can use it for this purpose:\n",
    "\n",
    "Class Imbalance: Examine the distribution of true positive and true negative values across classes. If the dataset is imbalanced, where one class has significantly more instances than others, the model may be biased towards predicting the majority class. This can lead to high accuracy but poor performance on the minority class.\n",
    "\n",
    "Precision and Recall Discrepancy: Check the precision and recall values for each class. If there is a significant difference between the two metrics for a particular class, it indicates that the model is either overpredicting or underpredicting that class. This could be due to biases in the data or issues in the model's ability to correctly handle that class.\n",
    "\n",
    "False Positives and False Negatives: Analyze the false positive and false negative values for each class. This can help you understand which classes the model is misclassifying and identify potential patterns or reasons behind those misclassifications.\n",
    "\n",
    "Confusion between Similar Classes: If the model frequently confuses two or more classes that are similar, it may indicate that the features used for distinguishing those classes are not distinct enough or that more data and better feature engineering are required.\n",
    "\n",
    "Threshold Adjustment: The default threshold for classifying instances as positive or negative is 0.5, but adjusting this threshold can impact the model's performance. For example, lowering the threshold may increase the recall at the cost of precision. Understanding the trade-offs can help identify the model's limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd3094-f69b-4bf1-bc49-4ec34adb1515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
